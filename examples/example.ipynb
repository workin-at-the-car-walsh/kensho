{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark pandas boto3 pyarrow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "sys.path.append('/home/darcy/kensho/') # add local library to path\n",
    "os.environ['SPARK_VERSION'] = '3.3' # required for deequ\n",
    "import kensho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "engine = kensho.ValidationEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no session yet\n",
      ":: loading settings :: url = jar:file:/home/darcy/spark3.3.2/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/darcy/.ivy2/cache\n",
      "The jars for the packages stored in: /home/darcy/.ivy2/jars\n",
      "com.amazon.deequ#deequ added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-2900da4d-70e4-4951-a3b7-d7ff68cacaa4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazon.deequ#deequ;2.0.4-spark-3.3 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.10 in central\n",
      "\tfound org.scalanlp#breeze_2.12;0.13.2 in central\n",
      "\tfound org.scalanlp#breeze-macros_2.12;0.13.2 in central\n",
      "\tfound com.github.fommil.netlib#core;1.1.2 in central\n",
      "\tfound net.sourceforge.f2j#arpack_combined_all;0.1 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.rwl#jtransforms;2.4.0 in central\n",
      "\tfound junit#junit;4.8.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.2 in central\n",
      "\tfound org.spire-math#spire_2.12;0.13.0 in central\n",
      "\tfound org.spire-math#spire-macros_2.12;0.13.0 in central\n",
      "\tfound org.typelevel#machinist_2.12;0.6.1 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.2 in central\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      ":: resolution report :: resolve 851ms :: artifacts dl 32ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazon.deequ#deequ;2.0.4-spark-3.3 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.2 from central in [default]\n",
      "\tcom.github.fommil.netlib#core;1.1.2 from central in [default]\n",
      "\tcom.github.rwl#jtransforms;2.4.0 from central in [default]\n",
      "\tjunit#junit;4.8.2 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\tnet.sourceforge.f2j#arpack_combined_all;0.1 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.10 from central in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;0.13.2 from central in [default]\n",
      "\torg.scalanlp#breeze_2.12;0.13.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\torg.spire-math#spire-macros_2.12;0.13.0 from central in [default]\n",
      "\torg.spire-math#spire_2.12;0.13.0 from central in [default]\n",
      "\torg.typelevel#machinist_2.12;0.6.1 from central in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.scala-lang#scala-reflect;2.12.1 by [org.scala-lang#scala-reflect;2.12.10] in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.0 by [org.scala-lang#scala-reflect;2.12.10] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   0   |   0   |   2   ||   17  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-2900da4d-70e4-4951-a3b7-d7ff68cacaa4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 17 already retrieved (0kB/18ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/23 14:22:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    print('no session yet')\n",
    "    \n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ValidationEngine\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.amazon.deequ:deequ:2.0.4-spark-3.3\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide spark session to engine\n",
    "engine.set_spark(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema for the check we are running\n",
    "# [rule_id, rule_type, comparison, threshold, alert_level]\n",
    "\n",
    "json_config = {\n",
    "    \"inputs\": {\n",
    "        \"example\": {\n",
    "        }\n",
    "    },\n",
    "    \"resolver_map\": {\n",
    "        \"example\": \"its_a_csv_right_here\"\n",
    "    },\n",
    "    \"loader_map\": {\n",
    "        \"example\": \"spark_read_csv\"\n",
    "    },\n",
    "    \"validation_object_loader_map\": {\n",
    "        \"example\": \"its_already_the_object\"\n",
    "    },\n",
    "    \"validation_objects\": {\n",
    "        \"example\": {\n",
    "            \"inputs\": [\"example\"]\n",
    "        }\n",
    "    },\n",
    "    \"validations\": {\n",
    "        \"example\": [[\"rule-001\", \"has_row_count\", \"ge\", 4, \"warning\"]]\n",
    "    }\n",
    "}\n",
    "app_parameters = {\n",
    "    \"s3_prefix\": None, \n",
    "    \"default_bucket\": None,\n",
    "    \"project_path\": None,\n",
    "    \"validation_dataset_name\": \"example\",\n",
    "    \"validation_dataset_bucket\": None\n",
    "\n",
    "}\n",
    "run_parameters = {\n",
    "    \"pipeline_execution_id\": None,\n",
    "    \"effective_extract_dt\": '20230822'\n",
    "}\n",
    "\n",
    "# load all required config to engine for the dataset we want to validate\n",
    "engine.load_from(json_config, app_parameters, run_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running entire engine\n",
      "current execution ID: manual-execution-1-20230824002257\n",
      "loading input object: example\n",
      "example.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darcy/my-python3-env/lib/python3.10/site-packages/pyspark/pandas/__init__.py:49: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated input objects\n",
      "dict_keys(['example'])\n",
      "loading validation object: example\n",
      "generated validation objects\n",
      "dict_keys(['example'])\n",
      "Python Callback server started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darcy/my-python3-env/lib/python3.10/site-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/home/darcy/my-python3-env/lib/python3.10/site-packages/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
      "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darcy/my-python3-env/lib/python3.10/site-packages/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  fields = [\n",
      "/home/darcy/my-python3-env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/darcy/my-python3-env/lib/python3.10/site-packages/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas DataFrame is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "/home/darcy/spark3.3.2/python/lib/pyspark.zip/pyspark/pandas/__init__.py:49: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was unable to order verbose results columns\n",
      "generated validation results\n",
      "dict_keys(['example'])\n",
      "finished\n",
      "Number of executions is now 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darcy/spark3.3.2/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# run the full validations using config provided\n",
    "engine.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule_id</th>\n",
       "      <th>check_level</th>\n",
       "      <th>check_status</th>\n",
       "      <th>constraint_status</th>\n",
       "      <th>validation_metric_value</th>\n",
       "      <th>rule_type</th>\n",
       "      <th>operator</th>\n",
       "      <th>threshold</th>\n",
       "      <th>alert_level</th>\n",
       "      <th>validation_dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rule-001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Success</td>\n",
       "      <td>5</td>\n",
       "      <td>has_row_count</td>\n",
       "      <td>ge</td>\n",
       "      <td>4.0</td>\n",
       "      <td>warning</td>\n",
       "      <td>example</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rule_id check_level check_status constraint_status  \\\n",
       "0  rule-001         NaN          NaN           Success   \n",
       "\n",
       "   validation_metric_value      rule_type operator  threshold alert_level  \\\n",
       "0                        5  has_row_count       ge        4.0     warning   \n",
       "\n",
       "  validation_dataset  \n",
       "0            example  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show verbose results dataframe\n",
    "engine.verbose_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/darcy/my-python3-env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/darcy/my-python3-env/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Generate a pandas DataFrame\n",
    "pdf = pd.DataFrame(np.random.rand(100, 3))\n",
    "\n",
    "# Create a Spark DataFrame from a pandas DataFrame using Arrow\n",
    "df = spark.createDataFrame(pdf)\n",
    "\n",
    "# Convert the Spark DataFrame back to a pandas DataFrame using Arrow\n",
    "result_pdf = df.select(\"*\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package           Version\n",
      "----------------- -----------\n",
      "asttokens         2.2.1\n",
      "backcall          0.2.0\n",
      "boto3             1.28.32\n",
      "botocore          1.31.32\n",
      "comm              0.1.4\n",
      "debugpy           1.6.7.post1\n",
      "decorator         5.1.1\n",
      "executing         1.2.0\n",
      "ipykernel         6.25.1\n",
      "ipython           8.14.0\n",
      "jedi              0.19.0\n",
      "jmespath          1.0.1\n",
      "jupyter_client    8.3.0\n",
      "jupyter_core      5.3.1\n",
      "matplotlib-inline 0.1.6\n",
      "nest-asyncio      1.5.7\n",
      "numpy             1.25.2\n",
      "packaging         23.1\n",
      "pandas            1.5.3\n",
      "parso             0.8.3\n",
      "pexpect           4.8.0\n",
      "pickleshare       0.7.5\n",
      "pip               22.0.2\n",
      "platformdirs      3.10.0\n",
      "prompt-toolkit    3.0.39\n",
      "psutil            5.9.5\n",
      "ptyprocess        0.7.0\n",
      "pure-eval         0.2.2\n",
      "py4j              0.10.9.5\n",
      "pyarrow           13.0.0\n",
      "pydeequ           1.1.0\n",
      "Pygments          2.16.1\n",
      "pyspark           3.3.2\n",
      "python-dateutil   2.8.2\n",
      "pytz              2023.3\n",
      "pyzmq             25.1.1\n",
      "s3transfer        0.6.2\n",
      "setuptools        59.6.0\n",
      "six               1.16.0\n",
      "stack-data        0.6.2\n",
      "tornado           6.3.3\n",
      "traitlets         5.9.0\n",
      "tzdata            2023.3\n",
      "urllib3           1.26.16\n",
      "wcwidth           0.2.6\n",
      "wheel             0.37.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install pandas==1.5.3\n",
    "# pyspark==3.3.2\n",
    "!pip list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-python3-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ab16126fba657b2102b2e3019b5debf19b3e6278427c2b39c6ed1ff8490cb8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
